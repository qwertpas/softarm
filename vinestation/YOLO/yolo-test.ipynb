{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15753427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLOWorld, YOLO\n",
    "import logging\n",
    "import serial\n",
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Load an official or custom model\n",
    "# model = YOLO(\"yolo11x.pt\")  # Load an official Detect model\n",
    "# model = YOLOWorld('yolov8x-worldv2.pt')\n",
    "# model.set_classes(['battery'])\n",
    "\n",
    "model = YOLO(\"yoloe-11s-seg.mlpackage\")\n",
    "\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = \"mps\"      # Apple Silicon GPU\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\"   # NVIDIA GPU\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# model = YOLO(\"yolo11n-seg.pt\")  # Load an official Segment model\n",
    "# model = YOLO(\"yolo11n-pose.pt\")  # Load an official Pose model\n",
    "# model = YOLO(\"path/to/best.pt\")  # Load a custom trained model\n",
    "\n",
    "# Perform tracking with the model\n",
    "# results = model.track(\"https://youtu.be/LNwODJXcvt4\", show=True)  # Tracking with default tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de69fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "port = '/dev/tty.usbmodem2101'\n",
    "serial_port = serial.Serial(port, 921600, timeout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b8130b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29761bed",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e830d12",
   "metadata": {},
   "source": [
    "## Capture Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cdf347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Photo Capture Tool\n",
      "Position your object in the frame and press SPACE to capture\n",
      "Press ESC to exit\n",
      "\n",
      "âœ… Photo saved: image.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "# Set the output file name heres\n",
    "filename = f\"image.jpg\"\n",
    "\n",
    "# Set up the camera\n",
    "cap = cv2.VideoCapture('http://192.168.1.27/stream')\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 800)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 800)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise\n",
    "\n",
    "print(\"Reference Photo Capture Tool\")\n",
    "print(\"Position your object in the frame and press SPACE to capture\")\n",
    "print(\"Press ESC to exit\")\n",
    "print()\n",
    "\n",
    "while True:\n",
    "    # Capture frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Resize to ensure 800x800 if the camera doesn't support it directly\n",
    "    frame = cv2.resize(frame, (800, 800))\n",
    "\n",
    "    # Add instructions on screen\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(frame, \"SPACE: Take Photo | ESC: Exit\", (10, 30),\n",
    "                font, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Capture Reference Photo\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # Capture photo on spacebar\n",
    "    if key == ord(' '):\n",
    "        # Save photo\n",
    "        cv2.imwrite(filename, frame)\n",
    "        print(f\"âœ… Photo saved: {filename}\")\n",
    "\n",
    "        # Show confirmation\n",
    "        confirm_frame = frame.copy()\n",
    "        cv2.putText(confirm_frame, f\"Saved: {filename}\", (10, 70),\n",
    "                    font, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Capture Reference Photo\", confirm_frame)\n",
    "        cv2.waitKey(2000)\n",
    "\n",
    "    # Exit on ESC\n",
    "    elif key == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6be1bb",
   "metadata": {},
   "source": [
    "## Draw Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f93b7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Bounding Box Tool - Loaded: image.jpg\n",
      "ðŸ“‹ Instructions:\n",
      "   â€¢ Click and drag to draw boxes around objects\n",
      "   â€¢ Each box gets a unique Class ID (0, 1, 2...)\n",
      "   â€¢ Press 'R' to reset all boxes\n",
      "   â€¢ Press 'Q' to quit\n",
      "\n",
      "ðŸ“¦ Box 1: [182, 302, 566, 729] (Class ID: 0)\n",
      "ðŸ”„ Reset all boxes\n",
      "ðŸ“¦ Box 1: [193, 347, 572, 719] (Class ID: 0)\n",
      "ðŸ”„ Reset all boxes\n",
      "ðŸ”„ Reset all boxes\n",
      "ðŸ“¦ Box 1: [188, 314, 499, 595] (Class ID: 0)\n",
      "ðŸ”„ Reset all boxes\n",
      "ðŸ“¦ Box 1: [192, 304, 570, 718] (Class ID: 0)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "# Set your image path here:\n",
    "image_path = \"image.jpg\"\n",
    "# ===================================\n",
    "\n",
    "class BoundingBoxTool:\n",
    "    def __init__(self, image_path):\n",
    "        self.image_path = image_path\n",
    "        self.image = cv2.imread(image_path)\n",
    "        if self.image is None:\n",
    "            raise ValueError(f\"âŒ Could not load image: {image_path}\")\n",
    "\n",
    "        self.original_image = self.image.copy()\n",
    "        self.boxes = []\n",
    "        self.current_box = []\n",
    "        self.drawing = False\n",
    "        self.class_id = 0\n",
    "        self.mouse_x = 0\n",
    "        self.mouse_y = 0\n",
    "\n",
    "        cv2.namedWindow(\"Bounding Box Tool\", cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.setMouseCallback(\"Bounding Box Tool\", self.mouse_callback)\n",
    "\n",
    "    def mouse_callback(self, event, x, y, flags, param):\n",
    "        self.mouse_x = x\n",
    "        self.mouse_y = y\n",
    "\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            self.drawing = True\n",
    "            self.current_box = [x, y]\n",
    "\n",
    "        elif event == cv2.EVENT_MOUSEMOVE:\n",
    "            if self.drawing:\n",
    "                temp_image = self.image.copy()\n",
    "                cv2.rectangle(temp_image, (self.current_box[0], self.current_box[1]),\n",
    "                            (x, y), (0, 255, 0), 2)\n",
    "                self.draw_crosshairs(temp_image, x, y)\n",
    "                cv2.imshow(\"Bounding Box Tool\", temp_image)\n",
    "            else:\n",
    "                temp_image = self.image.copy()\n",
    "                self.draw_crosshairs(temp_image, x, y)\n",
    "                cv2.imshow(\"Bounding Box Tool\", temp_image)\n",
    "\n",
    "        elif event == cv2.EVENT_LBUTTONUP:\n",
    "            self.drawing = False\n",
    "            self.current_box.extend([x, y])\n",
    "\n",
    "            # Create bounding box [x1, y1, x2, y2]\n",
    "            box = [\n",
    "                min(self.current_box[0], self.current_box[2]),\n",
    "                min(self.current_box[1], self.current_box[3]),\n",
    "                max(self.current_box[0], self.current_box[2]),\n",
    "                max(self.current_box[1], self.current_box[3])\n",
    "            ]\n",
    "\n",
    "            self.boxes.append({\n",
    "                'bbox': box,\n",
    "                'class_id': self.class_id\n",
    "            })\n",
    "\n",
    "            print(f\"ðŸ“¦ Box {len(self.boxes)}: {box} (Class ID: {self.class_id})\")\n",
    "            self.class_id += 1\n",
    "            self.update_display()\n",
    "\n",
    "    def draw_crosshairs(self, image, x, y):\n",
    "        height, width = image.shape[:2]\n",
    "        cv2.line(image, (x, 0), (x, height), (255, 255, 0), 1)\n",
    "        cv2.line(image, (0, y), (width, y), (255, 255, 0), 1)\n",
    "        cv2.putText(image, f\"({x}, {y})\", (x + 10, y - 10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "\n",
    "    def update_display(self):\n",
    "        self.image = self.original_image.copy()\n",
    "\n",
    "        # Draw all boxes\n",
    "        colors = [(0, 255, 0), (255, 0, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)]\n",
    "        for i, box_info in enumerate(self.boxes):\n",
    "            bbox = box_info['bbox']\n",
    "            class_id = box_info['class_id']\n",
    "            color = colors[class_id % len(colors)]\n",
    "\n",
    "            cv2.rectangle(self.image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)\n",
    "            cv2.putText(self.image, f\"ID:{class_id}\", (bbox[0], bbox[1]-10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "        # Add instructions\n",
    "        cv2.putText(self.image, \"R: Reset | Q: Quit\", (10, 60),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(self.image, f\"Next Class ID: {self.class_id}\", (10, 90),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        if hasattr(self, 'mouse_x') and hasattr(self, 'mouse_y'):\n",
    "            self.draw_crosshairs(self.image, self.mouse_x, self.mouse_y)\n",
    "\n",
    "        cv2.imshow(\"Bounding Box Tool\", self.image)\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"ðŸŽ¯ Bounding Box Tool - Loaded: {self.image_path}\")\n",
    "        print(\"ðŸ“‹ Instructions:\")\n",
    "        print(\"   â€¢ Click and drag to draw boxes around objects\")\n",
    "        print(\"   â€¢ Each box gets a unique Class ID (0, 1, 2...)\")\n",
    "        print(\"   â€¢ Press 'R' to reset all boxes\")\n",
    "        print(\"   â€¢ Press 'Q' to quit\")\n",
    "        print()\n",
    "\n",
    "        self.update_display()\n",
    "\n",
    "        while True:\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('r'):\n",
    "                self.boxes = []\n",
    "                self.class_id = 0\n",
    "                self.update_display()\n",
    "                print(\"ðŸ”„ Reset all boxes\")\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Check if image exists\n",
    "import os\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"âŒ Image not found: {image_path}\")\n",
    "    print(\"ðŸ“ Available images in current directory:\")\n",
    "    for file in os.listdir('.'):\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            print(f\"   {file}\")\n",
    "    exit()\n",
    "\n",
    "# Run the tool\n",
    "tool = BoundingBoxTool(image_path)\n",
    "tool.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e8106",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf95df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.234 ðŸš€ Python-3.11.14 torch-2.5.1 CPU (Apple M4 Pro)\n",
      "YOLOe-11s-seg summary (fused): 137 layers, 13,693,398 parameters, 1,857,958 gradients\n",
      "\n",
      "image 1/1 /Users/andrestorres/My Drive (actorres@ucsb.edu)/Fall 2025/ME225EH/YOLO/image.jpg: 640x640 1 object0, 79.7ms\n",
      "Speed: 1.2ms preprocess, 79.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Ultralytics 8.3.234 ðŸš€ Python-3.11.14 torch-2.5.1 CPU (Apple M4 Pro)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yoloe-11s-seg.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 38, 8400), (1, 32, 160, 160)) (26.5 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mCoreML:\u001b[0m starting export with coremltools 9.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 879/881 [00:00<00:00, 11988.32 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 236.45 passes/s]\n",
      "Running MIL default pipeline: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 133.76 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 246.08 passes/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mCoreML:\u001b[0m export success âœ… 3.6s, saved as 'yoloe-11s-seg.mlpackage' (19.4 MB)\n",
      "\n",
      "Export complete (3.7s)\n",
      "Results saved to \u001b[1m/Users/andrestorres/My Drive (actorres@ucsb.edu)/Fall 2025/ME225EH/YOLO\u001b[0m\n",
      "Predict:         yolo predict task=segment model=yoloe-11s-seg.mlpackage imgsz=640  \n",
      "Validate:        yolo val task=segment model=yoloe-11s-seg.mlpackage imgsz=640 data={'train': {'yolo_data': ['Objects365v1.yaml'], 'grounding_data': [{'img_path': '../datasets/flickr/full_images/', 'json_file': '../datasets/flickr/annotations/final_flickr_separateGT_train_segm.json'}, {'img_path': '../datasets/mixed_grounding/gqa/images', 'json_file': '../datasets/mixed_grounding/annotations/final_mixed_train_no_coco_segm.json'}]}, 'val': {'yolo_data': ['lvis.yaml']}}  \n",
      "Visualize:       https://netron.app\n",
      "Training complete!\n",
      "Object mapping:\n",
      "  ID 0: image.jpg\n",
      "  ID 1: battery.jpg\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLOE\n",
    "from ultralytics.models.yolo.yoloe import YOLOEVPSegPredictor\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "# Add as many objects as you want:\n",
    "training_data = [\n",
    "    {\n",
    "        \"image\": \"image.jpg\",\n",
    "        \"box\": [192, 304, 570, 718],\n",
    "    },\n",
    "    {\n",
    "        \"image\": \"battery.jpg\",\n",
    "        \"box\": [365, 315, 476, 520],\n",
    "    }\n",
    "]\n",
    "# ===================================\n",
    "\n",
    "model = YOLOE(\"yoloe-11s-seg.pt\")\n",
    "\n",
    "# Load all images and create a combined reference image\n",
    "images = [cv2.imread(data[\"image\"]) for data in training_data]\n",
    "\n",
    "# Stack images horizontally to create combined reference\n",
    "combined_image = np.hstack(images)\n",
    "\n",
    "# Adjust bounding boxes for the combined image\n",
    "all_bboxes = []\n",
    "all_class_ids = []\n",
    "x_offset = 0\n",
    "\n",
    "for i, (data, img) in enumerate(zip(training_data, images)):\n",
    "    box = data[\"box\"].copy() if isinstance(data[\"box\"], list) else list(data[\"box\"])\n",
    "    # Offset x coordinates by the cumulative width of previous images\n",
    "    adjusted_box = [box[0] + x_offset, box[1], box[2] + x_offset, box[3]]\n",
    "    all_bboxes.append(adjusted_box)\n",
    "    all_class_ids.append(i)\n",
    "    x_offset += img.shape[1]  # Add width of current image for next offset\n",
    "\n",
    "visual_prompts = {\n",
    "    'bboxes': np.array(all_bboxes),\n",
    "    'cls': np.array(all_class_ids)\n",
    "}\n",
    "\n",
    "# Train with all objects at once using combined reference image\n",
    "model.predict(\n",
    "    training_data[0][\"image\"],  # Can use any image for initial prediction\n",
    "    refer_image=combined_image,\n",
    "    visual_prompts=visual_prompts,\n",
    "    predictor=YOLOEVPSegPredictor,\n",
    "    conf=0.1\n",
    ")\n",
    "\n",
    "model.export(format=\"coreml\", imgsz=640)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(\"Object mapping:\")\n",
    "for i, data in enumerate(training_data):\n",
    "    print(f\"  ID {i}: {data['image']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b1ac6e",
   "metadata": {},
   "source": [
    "# Serial Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "102da1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serial_port.write('P9:0\\n'.encode())\n",
    "time.sleep(0.1)\n",
    "serial_port.write('P10:0\\n'.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d62d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serial_port.write('P10:1\\n'.encode())\n",
    "time.sleep(0.02)\n",
    "serial_port.write('P10:0\\n'.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4038c768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serial_port.write('P10:1\\n'.encode())\n",
    "time.sleep(0.01)\n",
    "serial_port.write('P9:0\\n'.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd841368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serial_port.write('P10:0\\n'.encode())\n",
    "time.sleep(0.02)\n",
    "serial_port.write('P9:0\\n'.encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee7ffb",
   "metadata": {},
   "source": [
    "# Websocket Servo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "21ed6586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"Client using the threading API.\"\"\"\n",
    "\n",
    "from websockets.sync.client import connect\n",
    "import json\n",
    "\n",
    "with connect(\"ws://192.168.1.27/websocket\") as websocket:\n",
    "    websocket.send(json.dumps({\"angle\": 0}))\n",
    "    # message = websocket.recv()\n",
    "    # print(message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211ae5e",
   "metadata": {},
   "source": [
    "# Predict\n",
    "(use this!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274c2489",
   "metadata": {},
   "source": [
    "0.2 for the area of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d87dd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:3@2624.470] global cap_ffmpeg_impl.hpp:453 _opencv_ffmpeg_interrupt_callback Stream timeout triggered after 30031.166046 ms\n",
      "[ WARN:3@2624.470] global cap_ffmpeg_impl.hpp:453 _opencv_ffmpeg_interrupt_callback Stream timeout triggered after 30031.314296 ms\n",
      "[ WARN:3@2624.470] global cap_ffmpeg_impl.hpp:453 _opencv_ffmpeg_interrupt_callback Stream timeout triggered after 30031.364046 ms\n",
      "[ WARN:3@2624.470] global cap_ffmpeg_impl.hpp:453 _opencv_ffmpeg_interrupt_callback Stream timeout triggered after 30031.397921 ms\n",
      "[ WARN:4@2624.493] global cap_ffmpeg_impl.hpp:453 _opencv_ffmpeg_interrupt_callback Stream timeout triggered after 30015.255339 ms\n",
      "[ WARN:4@2624.493] global cap_ffmpeg_impl.hpp:453 _opencv_ffmpeg_interrupt_callback Stream timeout triggered after 30015.374214 ms\n",
      "[ WARN:4@2624.493] global cap_ffmpeg_impl.hpp:453 _opencv_ffmpeg_interrupt_callback Stream timeout triggered after 30015.423131 ms\n",
      "[ WARN:4@2624.493] global cap_ffmpeg_impl.hpp:453 _opencv_ffmpeg_interrupt_callback Stream timeout triggered after 30015.462506 ms\n",
      "[tcp @ 0x10686fa40] Connection to tcp://192.168.1.27:80 failed: Operation timed out\n",
      "OpenCV: Couldn't read video stream from file \"http://192.168.1.27/stream\"\n",
      "[tcp @ 0x106a31000] Connection to tcp://192.168.1.27:80 failed: Operation timed out\n",
      "OpenCV: Couldn't read video stream from file \"http://192.168.1.27/stream\"\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from websockets import\n",
    "\n",
    "def direction(serial_: serial.Serial, state: Literal['left', 'right', 'middle'] = 'middle'):\n",
    "    # print(f'Moving: {state}    ', end='\\r')\n",
    "    if state == 'left':\n",
    "        serial_.write('P10:0\\n'.encode())\n",
    "        time.sleep(0.2)\n",
    "        serial_.write('P9:1\\n'.encode())\n",
    "    elif state == 'right':\n",
    "        serial_.write('P10:1\\n'.encode())\n",
    "        time.sleep(0.2)\n",
    "        serial_.write('P9:0\\n'.encode())\n",
    "    else:\n",
    "        serial_.write('P10:0\\n'.encode())\n",
    "        time.sleep(0.2)\n",
    "        serial_.write('P9:0\\n'.encode())\n",
    "\n",
    "state = 'middle'\n",
    "end = False\n",
    "\n",
    "def direction_thread():\n",
    "    global state\n",
    "    while not end:\n",
    "        direction(serial_port, state)\n",
    "        time.sleep(0.5)\n",
    "        direction(serial_port)\n",
    "        time.sleep(1)\n",
    "try:\n",
    "    logging.getLogger('ultralytics').setLevel(logging.ERROR)\n",
    "\n",
    "    with ThreadPoolExecutor() as executor, connect(\"ws://192.168.1.27/websocket\") as websocket:\n",
    "        executor.submit(direction_thread)\n",
    "        for result in model.predict(\n",
    "            'http://192.168.1.27/stream', stream=True,\n",
    "            device='mps',\n",
    "            verbose=False,\n",
    "        ):\n",
    "            frame = result.plot()\n",
    "\n",
    "            # Get inference time\n",
    "            text = f\"FPS: {1000 / result.speed['inference']:.1f}\"\n",
    "\n",
    "            # Define font and position\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            text_size = cv2.getTextSize(text, font, 1, 2)[0]\n",
    "            text_x = frame.shape[1] - text_size[0] - 10  # 10 pixels from the right\n",
    "            text_y = text_size[1] + 10  # 10 pixels from the top\n",
    "\n",
    "            cv2.putText(frame, text, (text_x, text_y), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            if result.boxes:\n",
    "                battery = result.boxes[0]\n",
    "                if battery:\n",
    "                    x, y, w, h = battery.xywhn.tolist()[0]\n",
    "\n",
    "                    area = w * h\n",
    "                    if area > 0.2:\n",
    "                        websocket.send(json.dumps({\"angle\": 0}))\n",
    "\n",
    "                    if x < 0.4:\n",
    "                        state = 'left'\n",
    "                    elif x > 0.6:\n",
    "                        state = 'right'\n",
    "                    else:\n",
    "                        state = 'middle'\n",
    "\n",
    "                    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                    text_size = cv2.getTextSize(state, font, 1, 2)[0]\n",
    "                    text_x = 10  # 10 pixels from the left\n",
    "                    text_y = text_size[1] + 10  # 10 pixels from the top\n",
    "\n",
    "                    cv2.putText(frame, f'{state} area: {w*h:.3f}', (text_x, text_y), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imshow(\"Battery Tracking\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                end = True\n",
    "                break\n",
    "finally:\n",
    "    end = True\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99e4e092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011dca7",
   "metadata": {},
   "source": [
    "# Tracking\n",
    "(don't use this it doesn't work super well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "try:\n",
    "    tracking_id = None\n",
    "    counter = itertools.count()\n",
    "    for result in model.track(\n",
    "        'http://192.168.1.27/stream', stream=True,\n",
    "        device='cpu',\n",
    "        verbose=False,\n",
    "        persist=True\n",
    "    ):\n",
    "        frame = result.plot()\n",
    "        if result.boxes and result.boxes.is_track:\n",
    "            if tracking_id is None:\n",
    "                tracking_id = result.boxes.id.int().cpu().tolist()[0]\n",
    "                print(f\"Initialized tracking ID: {tracking_id}\")\n",
    "            battery = result.boxes[result.boxes.id == tracking_id]\n",
    "            if battery:\n",
    "                x = battery.xywhn[0, 0]\n",
    "                if x < 0.4:\n",
    "                    print(\"Battery is on the left side.\")\n",
    "                elif x > 0.6:\n",
    "                    print(\"Battery is on the right side.\")\n",
    "            else:\n",
    "                if next(counter) % 30 == 0:\n",
    "                    print(\"Battery lost resetting the tracking id.\")\n",
    "                    tracking_id = None  # Reset tracking ID if battery not found for a while\n",
    "        cv2.imshow(\"Battery Tracking\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9935bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VideoCapture object\n",
    "# cap = cv2.VideoCapture('http://192.168.1.28/stream')\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the stream opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open MJPEG stream.\")\n",
    "else:\n",
    "    while True:\n",
    "        # Read a frame from the stream\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If frame is read correctly, ret is True\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame from stream. Exiting.\")\n",
    "            break\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('MJPEG Stream', frame)\n",
    "\n",
    "        # Press 'q' to exit the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the VideoCapture object and destroy all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8037e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 batterys, 46.3ms\n",
      "Speed: 3.3ms preprocess, 46.3ms inference, 35.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "success, frame = cap.read()\n",
    "result = model.track(frame, tracker=\"bytetrack.yaml\", device=device, persist=True)\n",
    "result.save('test.jpg')\n",
    "result.show()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d503e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open the video file\n",
    "# video_path = \"path/to/video.mp4\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Store the track history\n",
    "track_history = defaultdict(lambda: [])\n",
    "try:\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLO11 tracking on the frame, persisting tracks between frames\n",
    "        result = model.track(frame, tracker=\"bytetrack.yaml\", device=device, persist=True)[0]\n",
    "\n",
    "        # Get the boxes and track IDs\n",
    "        if result.boxes and result.boxes.is_track:\n",
    "            boxes = result.boxes.xywh.cpu()\n",
    "            track_ids = result.boxes.id.int().cpu().tolist()\n",
    "\n",
    "            # Visualize the result on the frame\n",
    "            frame = result.plot()\n",
    "\n",
    "            # Plot the tracks\n",
    "            for box, track_id in zip(boxes, track_ids):\n",
    "                x, y, w, h = box\n",
    "                track = track_history[track_id]\n",
    "                track.append((float(x), float(y)))  # x, y center point\n",
    "                if len(track) > 30:  # retain 30 tracks for 30 frames\n",
    "                    track.pop(0)\n",
    "\n",
    "                # Draw the tracking lines\n",
    "                points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "                cv2.polylines(frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLO11 Tracking\", frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "433a7393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43af1759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results[-2].boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53bddb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([0.], device='mps:0')\n",
       "conf: tensor([0.2499], device='mps:0')\n",
       "data: tensor([[6.5317e+02, 4.5461e+02, 8.4469e+02, 7.7824e+02, 2.4985e-01, 0.0000e+00]], device='mps:0')\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (1080, 1920)\n",
       "shape: torch.Size([1, 6])\n",
       "xywh: tensor([[748.9326, 616.4263, 191.5153, 323.6240]], device='mps:0')\n",
       "xywhn: tensor([[0.3901, 0.5708, 0.0997, 0.2997]], device='mps:0')\n",
       "xyxy: tensor([[653.1749, 454.6143, 844.6902, 778.2382]], device='mps:0')\n",
       "xyxyn: tensor([[0.3402, 0.4209, 0.4399, 0.7206]], device='mps:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-2].boxes[0]  # x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc8de1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vine-yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
